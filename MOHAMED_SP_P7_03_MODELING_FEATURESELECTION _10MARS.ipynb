{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6cf005",
   "metadata": {
    "id": "df6cf005"
   },
   "source": [
    "# Implémentez un modèle de scoring\n",
    "\n",
    "L'objectif est de développer un modèle de scoring de la probabilité de défaut d'un client\n",
    "\n",
    "Différents modèles de ML sont testés rapidement et le plus performant est sélectionné.\n",
    "\n",
    "Ensuite, une fonction d'optimisation via la librairie \"hyperopt\" est définie. Elle a permis d'optimiser des paramètres suivant deux métriques différentes: la métrique technique f1_score et une métrique bancaire custom_score faite sur mesure pour notre problème. Les paramètres optimisés sont ceux du modèle ML \n",
    "\n",
    "Dans une dernière partie, le modèle final ainsi que la méthode shap ont permis d'étudier quelles étaient les variables les plus influentes sur la prédiction de la solvabilité d'un client, d'un point de vue global puis local i.e pour un client spécifique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TCOW0iBrysQR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCOW0iBrysQR",
    "outputId": "5afa52a9-967d-44af-cfbf-af6ab43f4ab7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61eb3e0",
   "metadata": {
    "id": "e61eb3e0"
   },
   "source": [
    "## librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5be546",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unittest2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e576daf",
   "metadata": {
    "id": "1e576daf",
    "outputId": "ff6205b5-94e5-4084-9700-e703dba1ff63"
   },
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "920dd8ff",
   "metadata": {
    "id": "920dd8ff",
    "outputId": "647e689f-378a-44ba-a2fb-2c6ef90611de"
   },
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c46c85",
   "metadata": {
    "id": "08c46c85"
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import imblearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformations de variables\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Package pour augmenter la data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Metrics de ML\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, \\\n",
    "    fbeta_score, make_scorer,roc_curve,confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "# Packages de cross_validation\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Packages hyperopt pour la séléction d'hyperparamètres\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "# Modèles de ML\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "totABomJ0IG1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "totABomJ0IG1",
    "outputId": "cf1355d7-0176-4d52-d27f-768098fc2ce5"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install evidently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16VftQfwz5HS",
   "metadata": {
    "id": "16VftQfwz5HS"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import DataStabilityTestPreset\n",
    "\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ae670",
   "metadata": {
    "id": "ac1ae670"
   },
   "source": [
    "## chargement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c426d",
   "metadata": {
    "id": "c89c426d"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\Shata\\P7\\data.csv\")\n",
    "target=pd.read_csv(r\"C:\\Users\\Shata\\P7\\target.csv\")\n",
    "test=pd.read_csv(r\"C:\\Users\\Shata\\P7\\test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127df1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df, len(df)\n",
    "data, len_data = load_data_train(r\"C:\\Users\\Shata\\P7\\data.csv\")\n",
    "data, len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829af2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d77f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(unittest2.TestCase):\n",
    "    def test_load_data_train(self):\n",
    "        expected = 307507\n",
    "        df,len_ =  load_data_train(r\"C:\\Users\\Shata\\P7\\data.csv\")\n",
    "        self.assertEqual(len_,expected)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08450ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest2.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jvbtYTYS0oUw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jvbtYTYS0oUw",
    "outputId": "ccdabd02-1804-4110-cf25-b3b12363083a"
   },
   "outputs": [],
   "source": [
    "# permet de voir la ref avec la valeur actuelle avec les valeurs changé(modifié ou supprimer)--stabilite de donnees\n",
    "# mean stability est une metrique qui permet de voir la stabilite pour chaque col\n",
    "data_stability= TestSuite(tests=[\n",
    "    DataStabilityTestPreset(),\n",
    "])\n",
    "data_stability.run(current_data=data.iloc[:60], reference_data=data.iloc[60:], column_mapping=None)\n",
    "data_stability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NbXlPw450s3k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NbXlPw450s3k",
    "outputId": "f3a4348e-12a6-49c7-a26a-88136ca66f66"
   },
   "outputs": [],
   "source": [
    "\n",
    "# data_drift cest une etape dans process MLOPS qui permet d'identifier la différence entre une sorte de données dans des instant différents( elle permet de calculer le delta(difference) \n",
    "# de données en terme de stabilité de données suppression et ajout des features ,changement de type d'une colonne.)\n",
    "# MLOPS comporte un ensemble de outils qui permet d'effectuer un process bien precis,des outils sont:\n",
    "# MLFLOW, DATADRIFT, GITHUB, GITLAB(interface graphique pour visualiser le graph de commit de GIT ,dashboard qui permet de gere les anomalies,\n",
    " #                                 l'integration continue) et GIT( syteme de gestion de versionning pour le code)\n",
    "\n",
    "data_drift_report = Report(metrics=[\n",
    "    DataDriftPreset(),\n",
    "])\n",
    "\n",
    "data_drift_report.run(current_data=data.iloc[:60], reference_data=data.iloc[60:], column_mapping=None)\n",
    "data_drift_report"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aacba8bd",
   "metadata": {
    "id": "aacba8bd",
    "outputId": "5efe1057-dc03-45a2-f0f9-124f059e0fb3"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ed845",
   "metadata": {
    "id": "e92ed845"
   },
   "outputs": [],
   "source": [
    "#Set SK_ID_CURR as index\n",
    "data.set_index('SK_ID_CURR' ,inplace=True)\n",
    "test.set_index('SK_ID_CURR' ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5306c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "cae5306c",
    "outputId": "fcd5ff57-c381-4a1d-b743-9b72d2fbe46e"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(target.value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ea0d3",
   "metadata": {
    "id": "0f2ea0d3"
   },
   "outputs": [],
   "source": [
    "#24825/282682"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600081a9",
   "metadata": {
    "id": "600081a9"
   },
   "source": [
    "on observe un désequilibre de classe 92% et 8%..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4020824",
   "metadata": {
    "id": "f4020824"
   },
   "outputs": [],
   "source": [
    "# matrice X et y\n",
    "X = data\n",
    "y = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H2R2cjff-4Lz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2R2cjff-4Lz",
    "outputId": "b9004a01-22bc-41cb-810d-f0b86397875d"
   },
   "outputs": [],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3f044",
   "metadata": {
    "id": "f1a3f044"
   },
   "source": [
    "## Séléction du meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3143d2",
   "metadata": {
    "id": "1b3143d2"
   },
   "source": [
    "## Choix des métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197ab2d",
   "metadata": {
    "id": "3197ab2d"
   },
   "source": [
    "#### les défaillants forment la classe positive\n",
    "#### les non-défaillants forment la classe négative\n",
    "\n",
    "### Objectif de la modélisation :\n",
    "\n",
    "#### ne pas prédire un client non-défaillant s'il est défaillant ==> minimiser le nombre de faux négatifs (erreur de type II) (prédit non-défaillant mais client défaillant). Si un défaillant est prédit non défaillant, le groupe Home Crédit aura perdu toute la somme prêtée à l'emprunteur. Cela constitue les plus grosses pertes pour l'entreprise.\n",
    "\n",
    "#### ne pas prédire de défaillant si le client n'est pas défaillant donc minimiser les faux positifs (erreur de type I) (classe 1 défaillant alors que non-défaillant dans la réalité). Si un non-défaillant est prédit défaillant, le groupe Home Crédit aura perdu les intérêts de la somme prêtée à l'emprunteur.\n",
    "\n",
    "### métriques utiles \n",
    "\n",
    "#### Recall : la métrique pour déterminer le taux de vrais positif = TP/(TP+FN) --> maximiser\n",
    "#### Precision:  mesure le nombre d'observations prédites comme positives (client défaillant) qui le sont en réalité TP/(TP+FP) -->max\n",
    "#### F-mesure ou F1 : mix entre le recall et précision --> recall=fonction neg (precision) --> trouver le seuil métier...si logiquement la perte est plus severe quand on donne un credit a un defaillant : on priorise le recall (minimiser les FP) --> Fbeta >1 (moyenne harmonique)\n",
    "#### Score ROC AUC ; correlation de spearman entre le predit et la cible --> si le modele est ok\n",
    "#### Score PR AUC : air sous la courbe AUC ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88ed89",
   "metadata": {
    "id": "1f88ed89"
   },
   "outputs": [],
   "source": [
    "# Métriques\n",
    "roc_auc = make_scorer(roc_auc_score, greater_is_better=True,\n",
    "                      needs_proba=True)\n",
    "f1_score = make_scorer(fbeta_score, beta=1, greater_is_better=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404b97cf",
   "metadata": {
    "id": "404b97cf"
   },
   "source": [
    "## Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffac4e9",
   "metadata": {
    "id": "3ffac4e9"
   },
   "source": [
    "Nous allons tout d'abord voir les résultats entre des données balanced et unbalanced a travers une modelisation LGBM\n",
    "Nous utiliserons smot et procederons à un pipline afin d'éviter tout probleme de data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d299c1b",
   "metadata": {
    "id": "1d299c1b"
   },
   "outputs": [],
   "source": [
    "df_resultats = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e24529",
   "metadata": {
    "id": "c2e24529"
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395aa40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1395aa40",
    "outputId": "f67bd4c5-d566-4cf1-cd37-0df89af27a45"
   },
   "outputs": [],
   "source": [
    " !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ca766",
   "metadata": {
    "id": "b24ca766"
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcacf4",
   "metadata": {
    "id": "2bbcacf4"
   },
   "outputs": [],
   "source": [
    "# Cette fonction permet d'excuter un pipeline complet de processus de classification et l'affichage des résultats obtenu\n",
    "# pour les différents metrique (la surface sous la courbe ROC et la matrice de confusion)\n",
    "\n",
    "def process_classification(model, X,y,df_resultats, titre,smote=False, affiche_res=True,\n",
    "                           affiche_matrice_confusion=True):\n",
    "    # split data\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                       random_state=11)\n",
    "    \n",
    "    # inititalisation d'un pipeline pour le pretraitement de standardscale, SMOTE est un paramètre pour choisir les \n",
    "    # données aléatoire\n",
    "    if smote==True:\n",
    "        pipeline=imbpipeline(steps = [['smote', SMOTE(random_state=11)],\n",
    "                                        ['scaler', StandardScaler()],\n",
    "                                        ['classifier', model]])\n",
    "    else:\n",
    "       \n",
    "        pipeline = Pipeline(steps = [['scaler', StandardScaler()],\n",
    "                                     ['classifier', model]])\n",
    "\n",
    "    # Top début d'exécution\n",
    "    time_start = time.time()\n",
    "   \n",
    "    # Entraînement du modèle avec le jeu d'entraînement du jeu d'entrainement\n",
    "    # Definition d'une expérience (une expérience peut contenir plusieurs RAMS)\n",
    "    mlflow.set_experiment('experiences'+ titre)\n",
    "    # on fait appel à l'API mlflow qui intègre les algos de sklearn àfin d'enrégistrer tous les métriques de sklearn de métadonnées (mlflow tracking module)\n",
    "    mlflow.sklearn.autolog()\n",
    "    with mlflow.start_run(run_name=\"run\"+ titre):\n",
    "       pipeline.fit(X_train, y_train)\n",
    "   \n",
    "   # Prédictions avec le jeu de validation du jeu d'entraînement\n",
    "    model=pipeline\n",
    "    y_pred = model.predict(X_test)\n",
    " \n",
    "   # Top fin d'exécution\n",
    "    time_end = time.time()\n",
    "\n",
    "   # Probabilités\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    # Rappel/recall sensibilité\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Précision\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    # accuracy\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    # F-mesure ou Fbeta\n",
    "    f1_score = fbeta_score(y_test, y_pred, beta=1)\n",
    "    # Score ROC AUC aire sous la courbe ROC\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "   \n",
    "    # durée d'exécution entraînement + validation\n",
    "    time_execution = time_end - time_start\n",
    "\n",
    "  # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=11,shuffle=True)\n",
    "  # evaluate the model using cross-validation\n",
    "    scores = cross_val_score(pipeline, X_train, y_train,scoring='roc_auc', cv=cv)\n",
    "\n",
    " # Sauvegarde des performances\n",
    "    df_resultats = df_resultats.append(pd.DataFrame({\n",
    "        'Modèle': [titre],\n",
    "        'Rappel': [recall],\n",
    "        'Précision': [precision],\n",
    "        'Accuracy': [accuracy],\n",
    "        'F1': [f1_score],\n",
    "        'ROC_AUC': [roc_auc],\n",
    "        'Durée_tot': [time_execution]}), ignore_index=True)\n",
    "       \n",
    "    if affiche_res:\n",
    "        mask = df_resultats['Modèle'] == titre\n",
    "        display(df_resultats[mask].style.hide_index())\n",
    "\n",
    "    if affiche_matrice_confusion:\n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        plt.subplot(221)\n",
    "        y_pred= model.predict(X_test)\n",
    "        cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "        group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "        sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
    "        \n",
    "        # la surface de la courbe ROC (Faux positive  suivant x et le True positive suivant y)\n",
    "        plt.subplot(222)\n",
    "        y_true=y_test\n",
    "        y_pred_proba=model.predict_proba(X_test)[:,1]\n",
    "        fpr,tpr,_ = roc_curve(y_true,y_pred_proba )\n",
    "        plt.plot(fpr, tpr, color='orange', linewidth=5, label='AUC = %0.4f' %roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "\n",
    "       \n",
    "    return df_resultats, X_train, y_train, X_test, y_test, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01b829",
   "metadata": {
    "id": "de01b829"
   },
   "source": [
    "### LGBM\n",
    "Afin d'avoir une première idée des performances possibles, la modélisation par lgbm permettra d'obtenir une baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b3987",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "3e0b3987",
    "outputId": "4b83b346-9620-49f5-cf98-9582af8dbb09"
   },
   "outputs": [],
   "source": [
    "# SANS EQUILIBRAGE DES CLASSES\n",
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(LGBMClassifier(), X,\n",
    "                                        y,df_resultats,\n",
    "                                        'lgbm_unbalanced',smote=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5adedc",
   "metadata": {
    "id": "ec5adedc"
   },
   "source": [
    " fort déséquilibre entre la précision trouvée pour la Target 0 (0.92) et la Target 1 (0). \n",
    " résultat attendu car data n'est pas équilibré, avec 91% des individus classés en modalité 0 et 8.78% en modalité 1\n",
    "\n",
    "Il est donc intéressant de travailler cet Oversampling ) en ajustant la distribution de classe de manière à avoir une répartition plus égalitaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Ukj9g0sw38C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ukj9g0sw38C",
    "outputId": "590bb375-a0d9-4b82-894d-3331a8f90ea9"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xhb36kFGwrM2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhb36kFGwrM2",
    "outputId": "53341955-a317-49fe-db91-60fe784bd7e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyngrok import ngrok\n",
    "# Stopper les tunnels déja ouvère\n",
    "ngrok.kill()\n",
    "# setting auto_token from ngrok.com\n",
    "NGROK_AUTH_TOKEN = \"2MaZcwfLBUwZ4rD5c6CXVRS8ttN_4nLs7QescxxqpVaMQi4fr\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "# Open en http tunnel on port 5000 for ngrok website/Dashboard\n",
    "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
    "print(\"MLFLOW\", ngrok_tunnel.public_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WM-E0R5NwyN3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM-E0R5NwyN3",
    "outputId": "f17ffc33-06db-4b84-a864-52f2ed3b384c"
   },
   "outputs": [],
   "source": [
    "\n",
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51c536",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "6a51c536",
    "outputId": "0a7d5039-3264-400c-be84-66a7946ceb8f"
   },
   "outputs": [],
   "source": [
    "#  AVEC EQUILIBRAGE DES CLASSES \n",
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(LGBMClassifier(), X,\n",
    "                                        y,df_resultats,\n",
    "                                        'lgbm_balanced',smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76bff7",
   "metadata": {
    "id": "bd76bff7"
   },
   "source": [
    "Dans la suite de nos modélisations et a fin de pouvoir comparer les modèles nous ferons recour à smot pour tous les algorithmes étant donné que le data est desequilibré"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29a7a2",
   "metadata": {
    "id": "6e29a7a2"
   },
   "source": [
    "#### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41aca8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ab41aca8",
    "outputId": "f2e115dd-0945-4783-e1a9-581f608c7ab9"
   },
   "outputs": [],
   "source": [
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(LogisticRegression(), X,\n",
    "                                        y,df_resultats,\n",
    "                                        'lr_balanced',smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bUX1B3KtruO0",
   "metadata": {
    "id": "bUX1B3KtruO0"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9098fb62",
   "metadata": {
    "id": "9098fb62"
   },
   "source": [
    "#### Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c23a60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "83c23a60",
    "outputId": "1fc5906f-ab4c-4d0f-b541-47acc2f05ba2"
   },
   "outputs": [],
   "source": [
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(RandomForestClassifier(),X,\n",
    "                                        y,df_resultats,\n",
    "                                        'rfc_balanced',smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93768fb",
   "metadata": {
    "id": "d93768fb"
   },
   "source": [
    "#### MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2586e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cff2586e",
    "outputId": "3c11a77a-8041-4551-a35d-d70ed7f39f08"
   },
   "outputs": [],
   "source": [
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(MLPClassifier(),X,\n",
    "                                        y,df_resultats,\n",
    "                                        'dtc_balanced',smote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001a5ae",
   "metadata": {
    "id": "d001a5ae"
   },
   "source": [
    "#### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dce3c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "13dce3c5",
    "outputId": "fda9f66f-6ee1-4b23-ca5f-32ad8367f26a"
   },
   "outputs": [],
   "source": [
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(XGBClassifier(), X,\n",
    "                                        y,df_resultats,\n",
    "                                        'xgb_balanced',smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a708f",
   "metadata": {
    "id": "f42a708f"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64359b1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64359b1b",
    "outputId": "52833c5b-8151-4108-f7bc-71e8a8d89ad9"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "joblib.dump(model, 'statistique_1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frp-YZUB6Ngo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "frp-YZUB6Ngo",
    "outputId": "460c1c06-330e-4ce5-bf65-1be64f37ea73"
   },
   "outputs": [],
   "source": [
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf1be4",
   "metadata": {
    "id": "04cf1be4"
   },
   "source": [
    "#### Dummyclassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee7eb18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6ee7eb18",
    "outputId": "b3c836d3-10c7-4ec3-8f01-1c0cd8ba3f28"
   },
   "outputs": [],
   "source": [
    "df_resultats, X_train, y_train, X_test, y_test = process_classification(DummyClassifier(strategy = 'most_frequent'), X,\n",
    "                                        y,df_resultats,\n",
    "                                        'dc_balanced',smote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563bc0e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "563bc0e7",
    "outputId": "788c3a58-daaa-4c05-bb15-0915b875d0f5"
   },
   "outputs": [],
   "source": [
    "df_resultats.sort_values('ROC_AUC', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b8732",
   "metadata": {
    "id": "783b8732"
   },
   "source": [
    "LGBMClassifier reste le modèle le plus performant selon le couple AUC score/Time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8043960b",
   "metadata": {
    "id": "8043960b"
   },
   "source": [
    "## Fonction d'optimisation pour le choix des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e8fd22",
   "metadata": {
    "id": "56e8fd22"
   },
   "source": [
    "Notre choix s'est porté sur le LGBM: en effet, \"LightGBM est un cadre de boosting de gradient rapide, distribué et haute performance basé sur des algorithmes d'arbre de décision, utilisé pour le classement, la classification et de nombreuses autres tâches d'apprentissage automatique.\"\n",
    "\n",
    "Comment fonctionne le boosting ?\n",
    "\n",
    "\"Le boosting fonctionne sur le principe de l'amélioration des erreurs de l'apprenant précédent par l'intermédiaire de l'apprenant suivant.\"\n",
    "\n",
    "\"Dans l'apprentissage automatique et les statistiques, le taux d'apprentissage est un paramètre de réglage dans un algorithme d'optimisation qui détermine la taille du pas à chaque itération tout en se déplaçant vers un minimum d'une fonction de perte.\"\n",
    "\n",
    "Pour l'optimisation nous faisons le choix hyperopt car il présente un avantage de cout d'exécution et de possibilité que Gridsearch et Randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da719e",
   "metadata": {
    "id": "16da719e"
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6d500",
   "metadata": {
    "id": "81c6d500"
   },
   "outputs": [],
   "source": [
    "#Parameter \n",
    "params = {\n",
    "    'learning_rate':    hp.uniform('learning_rate',0.1,1),\n",
    "    'max_depth':        hp.choice('max_depth',np.arange(2, 100, 1, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 50, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree',0.4,1),\n",
    "    'subsample':        hp.uniform('subsample', 0.6, 1),\n",
    "    'num_leaves':       hp.choice('num_leaves', np.arange(1, 200, 1, dtype=int)),\n",
    "    'min_split_gain':   hp.uniform('min_split_gain', 0, 1),\n",
    "    'reg_alpha':        hp.uniform('reg_alpha',0,1),\n",
    "    'reg_lambda':       hp.uniform('reg_lambda',0,1),\n",
    "    'n_estimators':     400 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef9d0c",
   "metadata": {
    "id": "57ef9d0c"
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data,\n",
    "                                                        target.values,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                       random_state=11)\n",
    "    # pipeline\n",
    "    \n",
    "    pipeline=imbpipeline(steps = [['smote', SMOTE(random_state=11)],\n",
    "                                        ['scaler', StandardScaler()],\n",
    "                                        ['classifier', LGBMClassifier(**params)]])\n",
    "    #objective function to be minimized. \n",
    "    #Hyperopt will seek to minimize the loss returned by this function.\n",
    "    \n",
    "\n",
    "    cv = StratifiedKFold(5)\n",
    "    \n",
    "    score = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc',error_score='raise', cv=cv).mean()\n",
    "\n",
    "    \n",
    "  \n",
    "    loss = 1 - score    \n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda50bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffda50bc",
    "outputId": "d72baf5f-e1d6-4bf4-f807-d9a82baed2e3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best = fmin(fn=objective, space=params, max_evals=8, algo=tpe.suggest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40550b14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40550b14",
    "outputId": "4840634f-09fc-4688-f7c4-2dac74c7537c"
   },
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac6e1c",
   "metadata": {
    "id": "c2ac6e1c"
   },
   "outputs": [],
   "source": [
    "lgbm_optimise=LGBMClassifier(**best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341ddc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "3341ddc3",
    "outputId": "c42b791c-59f6-4db8-8c88-36f3a24e931f"
   },
   "outputs": [],
   "source": [
    "# modelisation avec les hyperparametres optimisés\n",
    "df_resultats, X_train, y_train, X_test, y_test, model = process_classification(lgbm_optimise, X,\n",
    "                                        y,df_resultats,\n",
    "                                        'lgbm_balanced_op',smote=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef5c576",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aef5c576",
    "outputId": "22b2e1a1-a705-45fa-b8b6-a84044872a62"
   },
   "outputs": [],
   "source": [
    "#Check list features importance \n",
    "sorted_idx = np.argsort(lgbm_optimise.feature_importances_)[::-1]\n",
    "for index in sorted_idx:\n",
    "    print([X_train.columns[index], lgbm_optimise.feature_importances_[index]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0aa97",
   "metadata": {
    "id": "2eb0aa97"
   },
   "source": [
    "### Seuil et fonction de cout\n",
    "La fonctionde coût est dérivée en répartissant un coût pour chaque faux positif et faux négatif et en obtenant le revenu global sur la base des prévisions correctes et incorrectes. Supposons les coûts et revenus suivants pour cet ensemble de données sur les défauts de paiement les interest moyen /customer qui rembourse =+10 principal remboursé moyen/customer =8 cout de traintement d'un risque = 3 cela veut dire que : FP : perte de 10 =-10 FN: perte de 8 =-8 TP: perte de 3 =-3 TN: gain de 10 =+10 gain/perte net = FP(-10)+FN(-8)+TP(-3)+TN(+10)\n",
    "\n",
    "sur la base de de cette fonction nous accordons arbitrairement les poids suivants:\n",
    "\n",
    "True negative = poids accordé au revenu des intérêts annuels moyen d'un client qui rembourse un prêt. +10\n",
    "\n",
    "True Positive = poids accordé à la perte évitée si on accorde un crédit à un client défaillant. +1\n",
    "\n",
    "False Positive = poids accordé au coût représenté par les revenus d'intérêts moyens perdus en prédisant un client en tant que défaillant, ne lui accordant pas le prêt. On considère que ce coût est de moindre importance que le coût d'un faux négatif, car la banque perd dans la cas de FP uniquement les intérêts alors qu'il peut perdre tout le capital dans le cadre de FN. On accorde donc un poids de -1\n",
    "\n",
    "False Negative = poids accordé au coût représenté par le montant moyen du capital perdu en accordant un prêt à un client défaillant. On considère que ce coût est x fois plus important que le coût des FP. On accorde donc un poids de -20 aux FN.\n",
    "\n",
    "Dans ce cas le Revenu net = (TN x 10) + (TP x 1) + (FP x -1) + (FN x -20)\n",
    "Ainsi nous pouvons jouer sur les coeff à partir de l'expertise métier. Pour la suite le choix sera arbitraire.\n",
    "On peut mettre en perspective et normaliser la fonction de revenu net avec la fonction de revenu net qui permet de produire le modèle prédisant les vraies classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172763a",
   "metadata": {
    "id": "f172763a"
   },
   "outputs": [],
   "source": [
    "# le choix des penalités est arbitraire et le but est de voir l'impact du seuil sur le gain et donc sur la matrice de confusion \n",
    "\n",
    "def custom_score(y_reel, y_pred, taux_tn=0, taux_fp=-1, taux_fn=-20, taux_tp=0):\n",
    "    # Matrice de Confusion\n",
    "    (tn, fp, fn, tp) = confusion_matrix(y_reel, y_pred).ravel()\n",
    "    # Gain total\n",
    "    gain_tot = tn * taux_tn + fp * taux_fp + fn * taux_fn + tp * taux_tp\n",
    "    # Gain maximum : toutes les prédictions sont correctes\n",
    "    gain_max = (fp + tn) * taux_tn + (fn + tp) * taux_tp\n",
    "    # Gain minimum : on accorde aucun prêt, la banque ne gagne rien\n",
    "    gain_min = (fp + tn) * taux_fp + (fn + tp) * taux_fn\n",
    "    \n",
    "    custom_score = (gain_tot - gain_min) / (gain_max - gain_min)\n",
    "    \n",
    "    # Gain normalisé (entre 0 et 1) un score élevé montre une meilleure\n",
    "    # performance\n",
    "    return custom_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60152115",
   "metadata": {
    "id": "60152115"
   },
   "outputs": [],
   "source": [
    "def determiner_seuil_probabilite(model, X_test, y_test, title, n=1):\n",
    "\n",
    "    seuils = np.arange(0, 1, 0.01)\n",
    "    sav_gains = []\n",
    " \n",
    "    for seuil in seuils:\n",
    "\n",
    "        # Score du modèle : n = 0 ou 1\n",
    "        y_proba = model.predict_proba(X_test)[:, n]\n",
    "\n",
    "        # Score > seuil de solvabilité : retourne 1 sinon 0\n",
    "        y_pred = (y_proba > seuil)\n",
    "        y_pred = np.multiply(y_pred, 1)\n",
    "        \n",
    "        # Sauvegarde du score de la métrique métier\n",
    "        sav_gains.append(custom_score(y_test, y_pred))\n",
    "    \n",
    "    df_score = pd.DataFrame({'Seuils' : seuils,\n",
    "                             'Gains' : sav_gains})\n",
    "    \n",
    "    # Score métrique métier maximal\n",
    "    gain_max = df_score['Gains'].max()\n",
    "    print(f'Score métrique métier maximal : {gain_max}')\n",
    "    # Seuil optimal pour notre métrique\n",
    "    seuil_max = df_score.loc[df_score['Gains'].argmax(), 'Seuils']\n",
    "    print(f'Seuil maximal : {seuil_max}')\n",
    "\n",
    "    # Affichage du gain en fonction du seuil de solvabilité\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(seuils, sav_gains)\n",
    "    plt.xlabel('Seuil de probabilité')\n",
    "    plt.ylabel('Métrique métier')\n",
    "    plt.title(title)\n",
    "    plt.xticks(np.linspace(0.1, 1, 10))\n",
    "    return  X_test, y_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32492dc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "32492dc6",
    "outputId": "a568703e-743a-4679-9f8a-9a453b96c57d"
   },
   "outputs": [],
   "source": [
    "X_test, y_test, model = determiner_seuil_probabilite(lgbm_optimise,\n",
    "                                          X, y,\n",
    "                                          'lgbm_optimise_seuils')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30d6715",
   "metadata": {
    "id": "a30d6715"
   },
   "source": [
    "### Scores du modèle en prenant en compte le seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883ba38",
   "metadata": {
    "id": "c883ba38"
   },
   "outputs": [],
   "source": [
    "def classification_seuil(model,seuil, X,y,\n",
    "                           df_resultats, titre,affiche_res=True,\n",
    "                           affiche_matrice_confusion=True):\n",
    "    # split data\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=y,\n",
    "                                                       random_state=11)\n",
    "\n",
    "   # Top début d'exécution\n",
    "    time_start = time.time()\n",
    "   # define the pipeline\n",
    "    pipe = Pipeline(steps = [('smote', SMOTE(random_state = 42)), \n",
    "                      ('standardscaler', MinMaxScaler()),\n",
    "                      ('model', model)])\n",
    "   \n",
    "   # Entraînement du modèle avec le jeu d'entraînement du jeu d'entrainement\n",
    "    pipe.fit(X_train, y_train)\n",
    "   \n",
    "   # Prédictions avec le jeu de validation du jeu d'entraînement\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    " \n",
    "\n",
    "   # Probabilités\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    # Prédictions avec le jeu de validation du jeu d'entraînement\n",
    "    # Score > seuil de probabilité : retourne 1 sinon 0\n",
    "    y_pred = (y_proba > seuil)\n",
    "    y_pred = np.multiply(y_pred, 1)\n",
    "    \n",
    "    # Top fin d'exécution\n",
    "    time_end = time.time()\n",
    "    \n",
    "    # Calcul des métriques\n",
    "    # Rappel/recall sensibilité\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Précision\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    # accuracy\n",
    "    accuracy=accuracy_score(y_test,y_pred)\n",
    "    # F-mesure ou Fbeta\n",
    "    f1_score = fbeta_score(y_test, y_pred, beta=1)\n",
    "    # Score ROC AUC aire sous la courbe ROC\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "   \n",
    "    # durée d'exécution entraînement + validation\n",
    "    time_execution = time_end - time_start\n",
    "\n",
    "  # define the evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits=5, random_state=1,shuffle=True)\n",
    "  # evaluate the model using cross-validation\n",
    "    scoring = ['roc_auc', 'recall', 'precision']\n",
    "    scores = cross_validate(pipe, X_train, y_train,scoring=scoring, cv=cv)\n",
    "\n",
    " # Sauvegarde des performances\n",
    "    df_resultats = df_resultats.append(pd.DataFrame({\n",
    "        'Modèle': [titre],\n",
    "        'Rappel': [recall],\n",
    "        'Précision': [precision],\n",
    "        'Accuracy': [accuracy],\n",
    "        'F1': [f1_score],\n",
    "        'ROC_AUC': [roc_auc],\n",
    "        'Durée_tot': [time_execution]}), ignore_index=True)\n",
    "       \n",
    "    if affiche_res:\n",
    "        mask = df_resultats['Modèle'] == titre\n",
    "        display(df_resultats[mask].style.hide_index())\n",
    "\n",
    "    if affiche_matrice_confusion:\n",
    "        fig = plt.figure(figsize=(20,15))\n",
    "        plt.subplot(221)\n",
    "        y_pred= model.predict(X_test)\n",
    "        cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "        group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "        sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')\n",
    "        \n",
    "        plt.subplot(222)\n",
    "        y_true=y_test\n",
    "        y_pred_proba=model.predict_proba(X_test)[:,1]\n",
    "        fpr,tpr,_ = roc_curve(y_true,y_pred_proba )\n",
    "        plt.plot(fpr, tpr, color='orange', linewidth=5, label='AUC = %0.4f' %roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "\n",
    "       \n",
    "    return df_resultats, model, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0133c28d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "0133c28d",
    "outputId": "bd61c205-6877-4b36-92fb-c6c3a1b8abe4"
   },
   "outputs": [],
   "source": [
    "df_resultats, model, X_trian, y_train, X_test, y_test = classification_seuil(lgbm_optimise, 0.07,X,y,df_resultats,\n",
    "                                            'lgbm_hyperopt_avec_seuil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454f46a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2454f46a",
    "outputId": "8f467c87-b0e8-40a9-b182-c2594566ad45"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'model_lgbm_opt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939949b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "4939949b",
    "outputId": "c570ab52-32a2-4c01-8ee4-b6ad0eb3992f"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model= joblib.load('model_lgbm_opt.pkl')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d12b41",
   "metadata": {
    "id": "b1d12b41"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf2d7b",
   "metadata": {
    "id": "6aaf2d7b"
   },
   "outputs": [],
   "source": [
    "# Sauvegarde du meilleur modèle \n",
    "with open('mypicklefile', 'wb') as best_model:\n",
    "    pickle.dump(lgbm_optimise, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd51812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "cdd51812",
    "outputId": "078e3664-77b2-44d1-db08-7ea728b4a32f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d997a",
   "metadata": {
    "id": "300d997a"
   },
   "source": [
    "## Interprétabilité du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf714b3",
   "metadata": {
    "id": "ecf714b3"
   },
   "source": [
    "### globale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423b42a",
   "metadata": {
    "id": "6423b42a"
   },
   "source": [
    "#### Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WfGEoVFOfEX5",
   "metadata": {
    "id": "WfGEoVFOfEX5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a9193",
   "metadata": {
    "id": "197a9193"
   },
   "outputs": [],
   "source": [
    "def plot_features_importance(features_importance, nom_variables,\n",
    "                             figsize=(6, 5) ):\n",
    "    \n",
    "    df_feat_imp = pd.DataFrame({'feature': nom_variables,\n",
    "                                'importance': features_importance})\n",
    "    df_feat_imp_tri = df_feat_imp.sort_values(by='importance')\n",
    "    \n",
    "    \n",
    "    # BarGraph de visalisation\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(df_feat_imp_tri['feature'], df_feat_imp_tri['importance'])\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.xlabel('Feature Importances (%)')\n",
    "    plt.ylabel('Variables', fontsize=10)\n",
    "    plt.title('Comparison des Features Importances', fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544092e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a544092e",
    "outputId": "38c63993-a891-4c38-9968-a517f0e222e2"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "feature_importance_values = lgbm_optimise.feature_importances_\n",
    "\n",
    "feature_names = list(X_train.columns)\n",
    "plot_features_importance(feature_importance_values,feature_names, (8, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8PWgVEEhXtP",
   "metadata": {
    "id": "a8PWgVEEhXtP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cfbc00a",
   "metadata": {
    "id": "0cfbc00a"
   },
   "source": [
    "### SHAP\n",
    "consiste à calculer la valeur de Shapley pour toutes les variables de tous les individus c’est-à-dire la moyenne de l’impact d’une variable (sur la sortie, donc la prédiction) pour toutes les combinaisons de variables possibles.\n",
    "La somme des effets de chaque variable expliquera la prédiction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f9820",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d9f9820",
    "outputId": "b0050383-9763-47cd-856a-8cd77b2f9604"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46605d19",
   "metadata": {
    "id": "46605d19"
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1230999",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1230999",
    "outputId": "926c3a24-7525-418d-932a-ccec3e56d92c"
   },
   "outputs": [],
   "source": [
    "# compute SHAP values\n",
    "explainer = shap.Explainer(lgbm_optimise, X_train)\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7afbc33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7afbc33",
    "outputId": "3be90d78-94ed-4b8f-ac0a-0c2977e47ef3"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a4bae",
   "metadata": {
    "id": "227a4bae"
   },
   "outputs": [],
   "source": [
    "# convertir le numpy array vers un pandas dataframe afin de pouvoir appliquer les méthodes de SHAP\n",
    "df_xtrain = pd.DataFrame(X_train, \n",
    "             columns=data.columns)\n",
    "\n",
    "df_xtest = pd.DataFrame(X_test, \n",
    "             columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e317fe9e",
   "metadata": {
    "id": "e317fe9e"
   },
   "outputs": [],
   "source": [
    "# compute SHAP values\n",
    "explainer = shap.Explainer(lgbm_optimise, df_xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed6eba2",
   "metadata": {
    "id": "7ed6eba2"
   },
   "outputs": [],
   "source": [
    "shap_values = explainer(df_xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51df306",
   "metadata": {
    "id": "c51df306"
   },
   "source": [
    "### Feature importance globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13956c0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "13956c0c",
    "outputId": "df978c7e-00a9-4f9a-91e8-0164be7e9e2a"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, \n",
    "                  df_xtest, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727281e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "1727281e",
    "outputId": "aee5d37b-428c-43c0-b4e1-58ae9e68bbf7"
   },
   "outputs": [],
   "source": [
    "# global feature importance\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b1784",
   "metadata": {
    "id": "d49b1784"
   },
   "source": [
    "### Feature importance locale\n",
    "#### Cas prêt accepté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b97d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa6b97d6",
    "outputId": "571e0f63-363c-48c6-97f1-51fb00609f78"
   },
   "outputs": [],
   "source": [
    "# probabilité de prediction du client N°0\n",
    "lgbm_optimise.predict_proba(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42db0a",
   "metadata": {
    "id": "1d42db0a"
   },
   "source": [
    " il y a une probabilité de 97% que la demande de prêt est acceptée (TARGET = 0)% que la  et 3% demande soit refusée (TARGET = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e96d39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8e96d39",
    "outputId": "e9b632bd-6214-4e63-d192-5874d3d5e6c7"
   },
   "outputs": [],
   "source": [
    "# prédiction du modèle\n",
    "lgbm_optimise.predict(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df3a8e9",
   "metadata": {
    "id": "2df3a8e9"
   },
   "source": [
    " Le résultat du client est TARGET =0 --> il y a pas de difficulté de remboursement du prêt --> acceptation du prêt.\n",
    "regardons la feature importance locale pour comprendre pourquoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b3a2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "637b3a2a",
    "outputId": "5e264886-9a0f-4bb0-8bfb-1d8262ea95c7"
   },
   "outputs": [],
   "source": [
    "# local feature importance pour le premier client\n",
    "# compute SHAP values\n",
    "shap.plots.bar(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d9681",
   "metadata": {
    "id": "771d9681"
   },
   "source": [
    "autrement 0.97= -0.75* feat1 -0.43* feat2+.....donc les valeurs de shapley indique que ce client est a risque à cause des coeff en rouge et non risqué a cause des coeff en bleus..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9619603",
   "metadata": {
    "id": "e9619603"
   },
   "source": [
    "#### cas pret refusé "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0f721",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9a0f721",
    "outputId": "2bb3cc2e-7463-4203-fdc7-4460b538e084"
   },
   "outputs": [],
   "source": [
    "# extraire les indices des clients dont leur pet est refusé\n",
    "x=lgbm_optimise.predict(X_test)\n",
    "x_refus = [i for i in range(len(x)) if x[i] == 1]\n",
    "x_refus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f5127d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64f5127d",
    "outputId": "918f48c0-b464-482a-abfa-f4f938aa7389"
   },
   "outputs": [],
   "source": [
    "# choisir client d'indice = 890\n",
    "lgbm_optimise.predict_proba(X_test)[890]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc91e22",
   "metadata": {
    "id": "cbc91e22"
   },
   "source": [
    "57% qu'il soit défaillant...voyons pourquoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c252c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "214c252c",
    "outputId": "60aed7f6-c8f7-45a6-81dd-2b21f7439e36"
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[890])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9df82",
   "metadata": {
    "id": "b1a9df82"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe12dc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "6fe12dc6",
    "outputId": "ac687da3-71e7-4424-9fd4-9530db145813"
   },
   "outputs": [],
   "source": [
    "X_train.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dzn-MEs06dlQ",
   "metadata": {
    "id": "dzn-MEs06dlQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iAG4cLgGwb2j",
   "metadata": {
    "id": "iAG4cLgGwb2j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ag7xJXv7wdvZ",
   "metadata": {
    "id": "ag7xJXv7wdvZ"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
